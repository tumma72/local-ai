# local-ai configuration example
# Copy to ~/.config/local-ai/config.toml or ./config.toml

[server]
# Host to bind the server
host = "127.0.0.1"
# Port to bind the server (1-65535)
port = 8080
# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
log_level = "INFO"

[model]
# Optional: Pre-load a model at server startup
# If not specified, models are loaded dynamically per-request
# Examples:
#   - "mlx-community/Qwen3-30B-A3B-4bit"
#   - "/path/to/local/model"
#   - "local/model-name" (for converted models in ~/.local/share/local-ai/models/)
# path = "mlx-community/Qwen3-30B-A3B-4bit"

# Optional: Path to LoRA adapter
# adapter_path = "/path/to/adapter"

# Trust remote code in tokenizer (use with caution)
# trust_remote_code = false

# Note: Generation settings (temperature, max_tokens, top_p, etc.) are configured
# in your client application (Zed, Claude Code, etc.) or per-request via the API.
# Use `local-ai models recommend <model-id>` to get recommended settings for a model.
